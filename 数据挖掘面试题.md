###SVM

1、支撑平面-和支持向量交互的平面,分割平面---支持平面中间面也就是最优分类平面

2、SVM不是定义损失,而是定义支持向量之间的距离目标函数

3、正则化参数对支持向量数的影响

###LR

1、LR的形式:h(x)=g(f(x)) 其中x为原数据,f(x)为线性/非线性回归得到的值,也叫判定边界 g()为Sigmod函数,最终h(x)输出的范围为(0,1)

LR对样本分布敏感

###LR和朴素贝叶斯(NB)之间的区别

LR是loss最优化求出的 NB是跳过统计Loss最优,直接得出权重的   NB比LR多了一个条件独立假设  LR属于判别模型 NB是生成模型

### 在机器学习中,LR和SVM有什么区别?

两者都可以处理非线性的问题;LR和SVM最初都是针对二分类问题的,SVM最大化间隔平面,LR极大似然估计,SVM只能输出类别,不能输出概率,两者LOSS function 不同,LR的可解释性更强,SVM自带有约束的正则化

### LR为什么用sigmod函数,这个函数有什么优点和缺点?为什么不用其他函数?

LR只能用于处理二分类,而Sigmod对于所有的输入,得到的输出接近0或者 1

Sigmod存在的问题,梯度消失、他的输出不是关于原点对称的导致收敛速度非常慢,计算非常耗时间

Tanh激活桉树存在的问题:梯度消失,计算耗时,但是其输出的是中心对称的

Relu:其输出不关于原点对称:反向传播时,输入的神经元小于0时,会有梯度消失问题,当x=0是,该点的梯度不存在(没有定义)

Relu问题:权重初始化不当,出事学习率设置的非常大

###SVM原问题和对偶问题关系？

SVM对偶问题的获得方法：将原问题的目标函数L和约束条件构造拉格朗日函数，再对L中原参数和lambda、miu分别求导，并且三种导数都等于0；再将等于0的三个导数带入原目标函数中，即可获得对偶问题的目标函数

关系：原问题的最大值相对于对偶问题的最小值

###KKT（Karysh-Kuhn-Tucker）条件有哪些，完整描述？

KKT条件是思考如何把约束优化转化为无约束优化à进而求约束条件的极值点

###决策树过拟合哪些方法，前后剪枝

决策树对训练属性有很好的分类能力；但对位置的测试数据未必有好的分类能力，泛化能力弱，即发生过拟合
防止过拟合的方法：剪枝（把一些相关的属性归为一个大类，减少决策树的分叉）；随机森林

###L1正则为什么可以把系数压缩成0，坐标回归的具体实现细节？

L1正则化可以实现稀疏（即截断），使训练得到的权重为0；

l1正则会产生稀疏解，即不相关的的特征对应的权重为0，就相当于降低了维度。但是l1的求解复杂度要高于l2,并且l1更为流行

正则化就是对loss进行惩罚（加了正则化项之后，使loss不可能为0,lambda越大惩罚越大-->lambda较小时，约束小，可能仍存在过拟合；太大时，使loss值集中于正则化的值上）

正则化使用方法：L1/L2/L1+L2

###LR在特征较多时可以进行怎样的优化？-->L1正则有特征选择的作用

如果是离线的话，L1正则可以有稀疏解，batch大点应该也有帮助，在线的解决思路有ftrl,rds,robots,还有阿里的mlr。当然还可以用gbdt,fm,ffm做一些特性选择和组合应该也有效果。

###机器学习里面的聚类和分类模型有哪些？

分类：LR、SVM、KNN、决策树、RandomForest、GBDT

回归：non-Linear regression、SVR（支持向量回归-->可用线性或高斯核（RBF））、随机森林

聚类：Kmeans、层次聚类、GMM（高斯混合模型）、谱聚类

聚类算法（可以作为监督学习中稀疏特征的处理）：Kmeans、层次聚类、GMM（高斯混合模型）

聚类算法唯一用到的信息是样本和样本之间的相似度。

评判聚类效果准则：高类间距，低类内距；高类内相似度，低类间相似度。

相似度与距离负相关。

图像之间的距离的度量是对每个像素操作，最后获得距离




###正则化为什么能防止过拟合？

过拟合表现在训练数据上的误差非常小，而在测试数据上误差反而增大。其原因一般是模型过于复杂，过分得去拟合数据的噪声. 正则化则是对模型参数添加先验，使得模型复杂度较小，对于噪声的输入扰动相对较小。

正则化时，相当于是给模型参数w 添加了一个协方差为1/lambda 的零均值高斯分布先

验。对于lambda =0，也就是不添加正则化约束，则相当于参数的高斯先验分布有

着无穷大的协方差，那么这个先验约束则会非常弱，模型为了拟合所有的训练数

据，w可以变得任意大不稳定。lambda越大，表明先验的高斯协方差越小，模型

约稳定，相对的variance(方差)也越小。

###SVM的核函数如何选取？

看到别人的答案里提到了吴恩达给出的选择核函数的方法，关于那三点，我的理解是：（1）如果特征维数很高，往往线性可分（SVM解决非线性分类问题的思路就是将样本映射到更高维的特征空间中），可以采用LR或者线性核的SVM；（2）如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；（3）如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核的SVM。

作者：杨宗霖
链接：https://www.zhihu.com/question/21883548/answer/205191440
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
### 特征选择的方法
3 [特征选择：](https://www.zhihu.com/question/28641663/answer/110165221)　当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。　　根据特征选择的形式又可以将特征选择方法分为3种：Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。　　我们使用sklearn中的feature_selection库来进行特征选择。

### GBDT和XGBoost区别
1.传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）；
2.传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；
3.XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，放置过拟合，这也是XGBoost优于传统GBDT的一个特性；
4.shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）；
5.列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过 拟合，还能减少计算；
6.对缺失值的处理。对于特征的值有缺失的样本，XGBoost还可以自动 学习出它的分裂方向；
7.XGBoost工具支持并行。Boosting不是一种串行的结构吗?怎么并行 的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

### [lr 和svm比较](https://zhuanlan.zhihu.com/p/28036014)
第一，本质上是其loss function不同。
第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用，虽然作用会相对小一些）。
第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。
第四，线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。
第五，SVM的目标函数就自带正则！！！（目标函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！




