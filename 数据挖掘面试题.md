###SVM

1、支撑平面-和支持向量交互的平面,分割平面---支持平面中间面也就是最优分类平面

2、SVM不是定义损失,而是定义支持向量之间的距离目标函数

3、正则化参数对支持向量数的影响

###LR

1、LR的形式:h(x)=g(f(x)) 其中x为原数据,f(x)为线性/非线性回归得到的值,也叫判定边界 g()为Sigmod函数,最终h(x)输出的范围为(0,1)

LR对样本分布敏感

###LR和朴素贝叶斯(NB)之间的区别

LR是loss最优化求出的 NB是跳过统计Loss最优,直接得出权重的   NB比LR多了一个条件独立假设  LR属于判别模型 NB是生成模型

### 在机器学习中,LR和SVM有什么区别?

两者都可以处理非线性的问题;LR和SVM最初都是针对二分类问题的,SVM最大化间隔平面,LR极大似然估计,SVM只能输出类别,不能输出概率,两者LOSS function 不同,LR的可解释性更强,SVM自带有约束的正则化

### LR为什么用sigmod函数,这个函数有什么优点和缺点?为什么不用其他函数?

LR只能用于处理二分类,而Sigmod对于所有的输入,得到的输出接近0或者 1

Sigmod存在的问题,梯度消失、他的输出不是关于原点对称的导致收敛速度非常慢,计算非常耗时间

Tanh激活桉树存在的问题:梯度消失,计算耗时,但是其输出的是中心对称的

Relu:其输出不关于原点对称:反向传播时,输入的神经元小于0时,会有梯度消失问题,当x=0是,该点的梯度不存在(没有定义)

Relu问题:权重初始化不当,出事学习率设置的非常大

###SVM原问题和对偶问题关系？

SVM对偶问题的获得方法：将原问题的目标函数L和约束条件构造拉格朗日函数，再对L中原参数和lambda、miu分别求导，并且三种导数都等于0；再将等于0的三个导数带入原目标函数中，即可获得对偶问题的目标函数

关系：原问题的最大值相对于对偶问题的最小值

###KKT（Karysh-Kuhn-Tucker）条件有哪些，完整描述？

KKT条件是思考如何把约束优化转化为无约束优化à进而求约束条件的极值点

###决策树过拟合哪些方法，前后剪枝

决策树对训练属性有很好的分类能力；但对位置的测试数据未必有好的分类能力，泛化能力弱，即发生过拟合
防止过拟合的方法：剪枝（把一些相关的属性归为一个大类，减少决策树的分叉）；随机森林

###L1正则为什么可以把系数压缩成0，坐标回归的具体实现细节？

L1正则化可以实现稀疏（即截断），使训练得到的权重为0；

l1正则会产生稀疏解，即不相关的的特征对应的权重为0，就相当于降低了维度。但是l1的求解复杂度要高于l2,并且l1更为流行

正则化就是对loss进行惩罚（加了正则化项之后，使loss不可能为0,lambda越大惩罚越大-->lambda较小时，约束小，可能仍存在过拟合；太大时，使loss值集中于正则化的值上）

正则化使用方法：L1/L2/L1+L2

###LR在特征较多时可以进行怎样的优化？-->L1正则有特征选择的作用

如果是离线的话，L1正则可以有稀疏解，batch大点应该也有帮助，在线的解决思路有ftrl,rds,robots,还有阿里的mlr。当然还可以用gbdt,fm,ffm做一些特性选择和组合应该也有效果。

###机器学习里面的聚类和分类模型有哪些？

分类：LR、SVM、KNN、决策树、RandomForest、GBDT

回归：non-Linear regression、SVR（支持向量回归-->可用线性或高斯核（RBF））、随机森林

聚类：Kmeans、层次聚类、GMM（高斯混合模型）、谱聚类

聚类算法（可以作为监督学习中稀疏特征的处理）：Kmeans、层次聚类、GMM（高斯混合模型）

聚类算法唯一用到的信息是样本和样本之间的相似度。

评判聚类效果准则：高类间距，低类内距；高类内相似度，低类间相似度。

相似度与距离负相关。

图像之间的距离的度量是对每个像素操作，最后获得距离




###正则化为什么能防止过拟合？

过拟合表现在训练数据上的误差非常小，而在测试数据上误差反而增大。其原因一般是模型过于复杂，过分得去拟合数据的噪声. 正则化则是对模型参数添加先验，使得模型复杂度较小，对于噪声的输入扰动相对较小。

正则化时，相当于是给模型参数w 添加了一个协方差为1/lambda 的零均值高斯分布先

验。对于lambda =0，也就是不添加正则化约束，则相当于参数的高斯先验分布有

着无穷大的协方差，那么这个先验约束则会非常弱，模型为了拟合所有的训练数

据，w可以变得任意大不稳定。lambda越大，表明先验的高斯协方差越小，模型

约稳定，相对的variance(方差)也越小。

###SVM的核函数如何选取？

看到别人的答案里提到了吴恩达给出的选择核函数的方法，关于那三点，我的理解是：（1）如果特征维数很高，往往线性可分（SVM解决非线性分类问题的思路就是将样本映射到更高维的特征空间中），可以采用LR或者线性核的SVM；（2）如果样本数量很多，由于求解最优化问题的时候，目标函数涉及两两样本计算内积，使用高斯核明显计算量会大于线性核，所以手动添加一些特征，使得线性可分，然后可以用LR或者线性核的SVM；（3）如果不满足上述两点，即特征维数少，样本数量正常，可以使用高斯核的SVM。

作者：杨宗霖
链接：https://www.zhihu.com/question/21883548/answer/205191440
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。
### 特征选择的方法
3 [特征选择：](https://www.zhihu.com/question/28641663/answer/110165221)　当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。　　根据特征选择的形式又可以将特征选择方法分为3种：Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。　　我们使用sklearn中的feature_selection库来进行特征选择。

### GBDT和XGBoost区别
1.传统的GBDT以CART树作为基学习器，XGBoost还支持线性分类器，这个时候XGBoost相当于L1和L2正则化的逻辑斯蒂回归（分类）或者线性回归（回归）；
2.传统的GBDT在优化的时候只用到一阶导数信息，XGBoost则对代价函数进行了二阶泰勒展开，得到一阶和二阶导数；
3.XGBoost在代价函数中加入了正则项，用于控制模型的复杂度。从权衡方差偏差来看，它降低了模型的方差，使学习出来的模型更加简单，放置过拟合，这也是XGBoost优于传统GBDT的一个特性；
4.shrinkage（缩减），相当于学习速率（XGBoost中的eta）。XGBoost在进行完一次迭代时，会将叶子节点的权值乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。（GBDT也有学习速率）；
5.列抽样。XGBoost借鉴了随机森林的做法，支持列抽样，不仅防止过 拟合，还能减少计算；
6.对缺失值的处理。对于特征的值有缺失的样本，XGBoost还可以自动 学习出它的分裂方向；
7.XGBoost工具支持并行。Boosting不是一种串行的结构吗?怎么并行 的？注意XGBoost的并行不是tree粒度的并行，XGBoost也是一次迭代完才能进行下一次迭代的（第t次迭代的代价函数里包含了前面t-1次迭代的预测值）。XGBoost的并行是在特征粒度上的。我们知道，决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），XGBoost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代 中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么各个特征的增益计算就可以开多线程进行。

### [lr 和svm比较](https://zhuanlan.zhihu.com/p/28036014)
第一，本质上是其loss function不同。
第二，支持向量机只考虑局部的边界线附近的点，而逻辑回归考虑全局（远离的点对边界线的确定也起作用，虽然作用会相对小一些）。
第三，在解决非线性问题时，支持向量机采用核函数的机制，而LR通常不采用核函数的方法。
第四，线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响。
第五，SVM的目标函数就自带正则！！！（目标函数中的1/2||w||^2项），这就是为什么SVM是结构风险最小化算法的原因！！！而LR必须另外在损失函数上添加正则项！！！

###　[xgboost原理](http://skyhigh233.com/blog/2016/12/01/gbdt-and-xgboost/)


###机器学习常见面试题整理
By Kubi Code
文章目录
1. 有监督学习和无监督学习的区别
2. 正则化
3. 过拟合
3.1. 产生的原因
3.2. 解决方法
4. 泛化能力
5. 生成模型和判别模型
6. 线性分类器与非线性分类器的区别以及优劣
6.1. 特征比数据量还大时，选择什么样的分类器？
6.2. 对于维度很高的特征，你是选择线性还是非线性分类器？
6.3. 对于维度极低的特征，你是选择线性还是非线性分类器？
7. ill-condition病态问题
8. L1和L2正则的区别，如何选择L1和L2正则
9. 特征向量的归一化方法
10. 特征向量的异常值处理
11. 越小的参数说明模型越简单
12. svm中rbf核函数与高斯和函数的比较
13. KMeans初始类簇中心点的选取
13.1. 选择批次距离尽可能远的K个点
13.2. 选用层次聚类或者Canopy算法进行初始聚类
14. ROC、AUC
14.1. ROC曲线
14.2. AUC
14.3. 为什么要使用ROC和AUC
15. 测试集和训练集的区别
16. 优化Kmeans
17. 数据挖掘和机器学习的区别
18. 备注
###1.有监督学习和无监督学习的区别
有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT）
无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)
###正则化.
正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。
###过拟合
如果一味的去提高训练数据的预测能力，所选模型的复杂度往往会很高，这种现象称为过拟合。所表现的就是模型训练时候的误差很小，但在测试的时候误差很大。
产生的原因
1.因为参数太多，会导致我们的模型复杂度上升，容易过拟合
2.权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征.
解决方法
1.交叉验证法
2.减少特征
3.正则化
4.权值衰减
5.验证数据
###泛化能力
泛化能力是指模型对未知数据的预测能力
###生成模型和判别模型
1.生成模型：由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。（朴素贝叶斯）
生成模型可以还原联合概率分布p(X,Y)，并且有较快的学习收敛速度，还可以用于隐变量的学习
2.判别模型：由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。（k近邻、决策树）
直接面对预测，往往准确率较高，直接对数据在各种程度上的抽象，所以可以简化模型
###线性分类器与非线性分类器的区别以及优劣
如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。
常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归
常见的非线性分类器：决策树、RF、GBDT、多层感知机
SVM两种都有(看线性核还是高斯核)

* 线性分类器速度快、编程方便，但是可能拟合效果不会很好
* 非线性分类器编程复杂，但是效果拟合能力强
特征比数据量还大时，选择什么样的分类器？
* 线性分类器，因为维度高的时候，数据一般在维度空间里面会比较稀疏，很有可能	线性可分
###对于维度很高的特征，你是选择线性还是非线性分类器？
理由同上
###对于维度极低的特征，你是选择线性还是非线性分类器？
非线性分类器，因为低维空间可能很多特征都跑到一起了，导致线性不可分
###ill-condition病态问题
训练完的模型测试样本稍作修改就会得到差别很大的结果，就是病态问题（这简直是不能用啊）
###L1和L2正则的区别，如何选择L1和L2正则
他们都是可以防止过拟合，降低模型复杂度
L1是在loss function后面加上 模型参数的1范数（也就是|xi|）
L2是在loss function后面加上 模型参数的2范数（也就是sigma(xi^2)），注意L2范数的定义是sqrt(sigma(xi^2))，在正则项上没有添加sqrt根号是为了更加容易优化
L1 会产生稀疏的特征
L2 会产生更多地特征但是都会接近于0
L1会趋向于产生少量的特征，而其他的特征都是0，而L2会选择更多的特征，这些特征都会接近于0。L1在特征选择时候非常有用，而L2就只是一种规则化而已。
###特征向量的归一化方法

* 线性函数转换，表达式如下：y=(x-MinValue)/(MaxValue-MinValue)
* 对数函数转换，表达式如下：y=log10 (x)
* 反余切函数转换 ，表达式如下：y=arctan(x)*2/PI
* 减去均值，乘以方差：y=(x-means)/ variance
###特征向量的异常值处理
1.用均值或者其他统计量代替
越小的参数说明模型越简单
过拟合的，拟合会经过曲面的每个点，也就是说在较小的区间里面可能会有较大的曲率，这里的导数就是很大，线性模型里面的权值就是导数，所以越小的参数说明模型越简单。
追加：这个其实可以看VC维相关的东西感觉更加合适
svm中rbf核函数与高斯和函数的比较
高斯核函数好像是RBF核的一种
###KMeans初始类簇中心点的选取
选择批次距离尽可能远的K个点
首先随机选取一个点作为初始点，然后选择距离与该点最远的那个点作为中心点，再选择距离与前两个点最远的店作为第三个中心店，以此类推，直至选取大k个
选用层次聚类或者Canopy算法进行初始聚类
###ROC、AUC
ROC和AUC通常是用来评价一个二值分类器的好坏
ROC曲线
曲线坐标上：
X轴是FPR（表示假阳率-预测结果为positive，但是实际结果为negitive，FP/(N)）
Y轴式TPR（表示真阳率-预测结果为positive，而且的确真实结果也为positive的,TP/P）
那么平面的上点(X,Y)：
(0,1)表示所有的positive的样本都预测出来了，分类效果最好
(0,0)表示预测的结果全部为negitive
(1,0)表示预测的错过全部分错了，分类效果最差
(1,1)表示预测的结果全部为positive
针对落在x=y上点，表示是采用随机猜测出来的结果

ROC曲线建立
一般默认预测完成之后会有一个概率输出p，这个概率越高，表示它对positive的概率越大。
现在假设我们有一个threshold，如果p>threshold，那么该预测结果为positive，否则为negitive，按照这个思路，我们多设置几个threshold,那么我们就可以得到多组positive和negitive的结果了，也就是我们可以得到多组FPR和TPR值了
将这些(FPR,TPR)点投射到坐标上再用线连接起来就是ROC曲线了
当threshold取1和0时，分别得到的就是(0,0)和(1,1)这两个点。（threshold=1，预测的样本全部为负样本，threshold=0，预测的样本全部为正样本）
AUC
AUC(Area Under Curve)被定义为ROC曲线下的面积，显然这个面积不会大于1（一般情况下ROC会在x=y的上方，所以0.5<AUC<1）.
AUC越大说明分类效果越好
为什么要使用ROC和AUC
因为当测试集中的正负样本发生变化时，ROC曲线能基本保持不变，但是precision和recall可能就会有较大的波动。
http://www.douban.com/note/284051363/?type=like
###测试集和训练集的区别
训练集用于建立模型,测试集评估模型的预测等能力
优化Kmeans
使用kd树或者ball tree(这个树不懂)
将所有的观测实例构建成一颗kd树，之前每个聚类中心都是需要和每个观测点做依次距离计算，现在这些聚类中心根据kd树只需要计算附近的一个局部区域即可
###数据挖掘和机器学习的区别
机器学习是数据挖掘的一个重要工具，但是数据挖掘不仅仅只有机器学习这一类方法，还有其他很多非机器学习的方法，比如图挖掘，频繁项挖掘等。感觉数据挖掘是从目的而言的，但是机器学习是从方法而言的。

###[牛客网面经](https://www.nowcoder.com/discuss/33958?type=2&order=0&pos=11&page=1)






